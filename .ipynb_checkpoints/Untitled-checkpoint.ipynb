{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)? (<ipython-input-1-c7a90bf41e6f>, line 83)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-c7a90bf41e6f>\"\u001b[1;36m, line \u001b[1;32m83\u001b[0m\n\u001b[1;33m    print \"Successfully loaded:\", checkpoint.model_checkpoint_path\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)?\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# File: Deep Q-Learning Algorithm\n",
    "# Author: Flood Sung\n",
    "# Date: 2016.3.21\n",
    "# https://blog.csdn.net/songrotek/article/details/50951537\n",
    "# -----------------------------\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import random\n",
    "from collections import deque \n",
    "\n",
    "class BrainDQN:\n",
    "\n",
    "    # Hyper Parameters:\n",
    "    ACTION = 2\n",
    "    FRAME_PER_ACTION = 1\n",
    "    GAMMA = 0.99 # decay rate of past observations\n",
    "    OBSERVE = 100000. # timesteps to observe before training\n",
    "    EXPLORE = 150000. # frames over which to anneal epsilon\n",
    "    FINAL_EPSILON = 0.0 # final value of epsilon\n",
    "    INITIAL_EPSILON = 0.0 # starting value of epsilon\n",
    "    REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "    BATCH_SIZE = 32 # size of minibatch\n",
    "\n",
    "    def __init__(self):\n",
    "        # init replay memory\n",
    "        self.replayMemory = deque()\n",
    "        # init Q network\n",
    "        self.createQNetwork()\n",
    "        # init some parameters\n",
    "        self.timeStep = 0\n",
    "        self.epsilon = self.INITIAL_EPSILON\n",
    "\n",
    "    def createQNetwork(self):\n",
    "        # network weights\n",
    "        W_conv1 = self.weight_variable([8,8,4,32])\n",
    "        b_conv1 = self.bias_variable([32])\n",
    "\n",
    "        W_conv2 = self.weight_variable([4,4,32,64])\n",
    "        b_conv2 = self.bias_variable([64])\n",
    "\n",
    "        W_conv3 = self.weight_variable([3,3,64,64])\n",
    "        b_conv3 = self.bias_variable([64])\n",
    "\n",
    "        W_fc1 = self.weight_variable([1600,512])\n",
    "        b_fc1 = self.bias_variable([512])\n",
    "\n",
    "        W_fc2 = self.weight_variable([512,self.ACTION])\n",
    "        b_fc2 = self.bias_variable([self.ACTION])\n",
    "\n",
    "        # input layer\n",
    "\n",
    "        self.stateInput = tf.placeholder(\"float\",[None,80,80,4])\n",
    "\n",
    "        # hidden layers\n",
    "        h_conv1 = tf.nn.relu(self.conv2d(self.stateInput,W_conv1,4) + b_conv1)\n",
    "        h_pool1 = self.max_pool_2x2(h_conv1)\n",
    "\n",
    "        h_conv2 = tf.nn.relu(self.conv2d(h_pool1,W_conv2,2) + b_conv2)\n",
    "\n",
    "        h_conv3 = tf.nn.relu(self.conv2d(h_conv2,W_conv3,1) + b_conv3)\n",
    "\n",
    "        h_conv3_flat = tf.reshape(h_conv3,[-1,1600])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat,W_fc1) + b_fc1)\n",
    "\n",
    "        # Q Value layer\n",
    "        self.QValue = tf.matmul(h_fc1,W_fc2) + b_fc2\n",
    "\n",
    "        self.actionInput = tf.placeholder(\"float\",[None,self.ACTION])\n",
    "        \n",
    "        self.yInput = tf.placeholder(\"float\", [None])\n",
    "        Q_action = tf.reduce_sum(tf.mul(self.QValue, self.actionInput), reduction_indices = 1)\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.square(self.yInput - Q_action))\n",
    "        self.trainStep = tf.train.AdamOptimizer(1e-6).minimize(self.cost)\n",
    "\n",
    "        # saving and loading networks\n",
    "        saver = tf.train.Saver()\n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.session.run(tf.initialize_all_variables())\n",
    "        checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "                saver.restore(self.session, checkpoint.model_checkpoint_path)\n",
    "                print \"Successfully loaded:\", checkpoint.model_checkpoint_path\n",
    "        else:\n",
    "                print \"Could not find old network weights\"\n",
    "\n",
    "    def trainQNetwork(self):\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        minibatch = random.sample(self.replayMemory, self.BATCH_SIZE)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        nextState_batch = [data[3] for data in minibatch]\n",
    "\n",
    "        # Step 2: calculate y \n",
    "        y_batch = []\n",
    "        QValue_batch = self.QValue.eval(feed_dict={self.stateInput:nextState_batch})\n",
    "        for i in range(0,self.BATCH_SIZE):\n",
    "            terminal = minibatch[i][4]\n",
    "            if terminal:\n",
    "                y_batch.append(reward_batch[i])\n",
    "            else:\n",
    "                y_batch.append(reward_batch[i] + GAMMA * np.max(QValue_batch[i]))\n",
    "\n",
    "        self.trainStep.run(feed_dict={\n",
    "            self.yInput : y_batch,\n",
    "            self.actionInput : action_batch,\n",
    "            self.stateInput : state_batch\n",
    "            })\n",
    "\n",
    "        # save network every 100000 iteration\n",
    "        if self.timeStep % 10000 == 0:\n",
    "            saver.save(self.session, 'saved_networks/' + 'network' + '-dqn', global_step = self.timeStep)\n",
    "\n",
    "\n",
    "    def setPerception(self,nextObservation,action,reward,terminal):\n",
    "#         newState = np.append(nextObservation,self.currentState[:,:,1:],axis = 2)\n",
    "        newState = np.append(nextObservation, self.currentState[:,:,:3], axis = 2)\n",
    "        self.replayMemory.append((self.currentState, action, reward, newState, terminal))\n",
    "        if len(self.replayMemory) > self.REPLAY_MEMORY:\n",
    "            self.replayMemory.popleft()\n",
    "        if self.timeStep > self.OBSERVE:\n",
    "            # Train the network\n",
    "            self.trainQNetwork()\n",
    "\n",
    "        self.currentState = newState\n",
    "        self.timeStep += 1\n",
    "\n",
    "    def getAction(self):\n",
    "        QValue = self.QValue.eval(feed_dict= {self.stateInput:[self.currentState]})[0]\n",
    "        action = np.zeros(self.ACTION)\n",
    "        action_index = 0\n",
    "        if self.timeStep % self.FRAME_PER_ACTION == 0:\n",
    "            if random.random() <= self.epsilon:\n",
    "                action_index = random.randrange(self.ACTION)\n",
    "                action[action_index] = 1\n",
    "            else:\n",
    "                action_index = np.argmax(QValue)\n",
    "                action[action_index] = 1\n",
    "        else:\n",
    "            action[0] = 1 # do nothing\n",
    "\n",
    "        # change episilon\n",
    "        if self.epsilon > self.FINAL_EPSILON and self.timeStep > self.OBSERVE:\n",
    "            self.epsilon -= (self.INITIAL_EPSILON - self.FINAL_EPSILON)/self.EXPLORE\n",
    "\n",
    "        return action\n",
    "\n",
    "    def setInitState(self,observation):\n",
    "        self.currentState = np.stack((observation, observation, observation, observation), axis = 2)\n",
    "\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.01)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.01, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(self,x, W, stride):\n",
    "        return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "\n",
    "    def max_pool_2x2(self,x):\n",
    "        return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(a):\n",
    "    a += 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Project: Deep Q-Learning on Flappy Bird\n",
    "# Author: Flood Sung\n",
    "# Date: 2016.3.21\n",
    "# -------------------------\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "import wrapped_flappy_bird as game\n",
    "from BrainDQN import BrainDQN\n",
    "import numpy as np\n",
    "\n",
    "# preprocess raw image to 80*80 gray image\n",
    "def preprocess(observation):\n",
    "    observation = cv2.cvtColor(cv2.resize(observation, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY)\n",
    "    return np.reshape(observation,(80,80,1))\n",
    "\n",
    "def playFlappyBird():\n",
    "    # Step 1: init BrainDQN\n",
    "    brain = BrainDQN()\n",
    "    # Step 2: init Flappy Bird Game\n",
    "    flappyBird = game.GameState()\n",
    "    # Step 3: play game\n",
    "    # Step 3.1: obtain init state\n",
    "    action0 = np.array([1,0])  # do nothing\n",
    "    observation0, reward0, terminal = flappyBird.frame_step(action0)\n",
    "    observation0 = cv2.cvtColor(cv2.resize(observation0, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, observation0 = cv2.threshold(observation0,1,255,cv2.THRESH_BINARY)\n",
    "    brain.setInitState(observation0)\n",
    "\n",
    "    # Step 3.2: run the game\n",
    "    while 1!= 0:\n",
    "        action = brain.getAction()\n",
    "        nextObservation,reward,terminal = flappyBird.frame_step(action)\n",
    "        nextObservation = preprocess(nextObservation)\n",
    "        brain.setPerception(nextObservation,action,reward,terminal)\n",
    "\n",
    "def main():\n",
    "    playFlappyBird()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
